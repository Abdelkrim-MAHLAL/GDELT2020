{"paragraphs":[{"text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._\nimport sqlContext.implicits._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.sql.SQLContext\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport org.apache.spark.sql.types.IntegerType\nimport com.amazonaws.services.s3.AmazonS3Client\n\n","user":"anonymous","dateUpdated":"2020-03-04T15:11:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql.functions._\nimport sqlContext.implicits._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.sql.SQLContext\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport org.apache.spark.sql.types.IntegerType\nimport com.amazonaws.services.s3.AmazonS3Client\n"}]},"apps":[],"jobName":"paragraph_1583324719299_-1438052335","id":"20200228-120536_1109407589","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T15:11:31+0000","dateFinished":"2020-03-04T15:11:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:16720"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583324991456_545616681","id":"20200304-122951_810668010","dateCreated":"2020-03-04T12:29:51+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:16721"},{"text":"%pyspark\nimport urllib\n\nprint('Beginning file download with urllib2...')\n\nurl = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt'\nurllib.urlretrieve(url, '/tmp/masterfilelist.txt')\n\n    ","user":"anonymous","dateUpdated":"2020-03-04T12:30:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Beginning file download with urllib2...\n('/tmp/masterfilelist.txt', <httplib.HTTPMessage instance at 0x7f7d4d216bd8>)\n"}]},"apps":[],"jobName":"paragraph_1583324719300_1020707749","id":"20200228-125452_27512733","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:30:11+0000","dateFinished":"2020-03-04T12:30:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16722"},{"text":"%pyspark\nimport urllib\n\nprint('Beginning file download with urllib2...')\n\nurl = 'http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\"'\nurllib.urlretrieve(url, '/tmp/masterfilelist_translation.txt')","user":"anonymous","dateUpdated":"2020-03-04T12:30:27+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Beginning file download with urllib2...\n('/tmp/masterfilelist_translation.txt', <httplib.HTTPMessage instance at 0x7f7d4d2168c0>)\n"}]},"apps":[],"jobName":"paragraph_1583324719300_-247261553","id":"20200228-132837_1990250846","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:30:27+0000","dateFinished":"2020-03-04T12:30:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16723"},{"text":"%pyspark\nfrom pyspark.sql import Row\ndata_file = sc.textFile('s3://gdelt2020/masterfilelist.txt')\ntextSplit = data_file.map(lambda line : line.split(\" \"))\ndataRDD = textSplit.map(lambda item : Row (size=item[0], hash=item[1], url=item[2]))\nprint(\"succes\")\n","user":"anonymous","dateUpdated":"2020-03-04T12:30:33+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"succes\n"}]},"apps":[],"jobName":"paragraph_1583324719301_-121679781","id":"20200228-133842_832174065","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:30:33+0000","dateFinished":"2020-03-04T12:30:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16724"},{"text":"%pyspark\nfrom pyspark.sql import Row\ndata_file = sc.textFile('s3://gdelt2020/masterfilelist_translation.txt')\ntextSplit = data_file.map(lambda line : line.split(\" \"))\ndataRDD_translation = textSplit.map(lambda item : Row (size=item[0], hash=item[1], url=item[2]))\nprint(\"succes\")","user":"anonymous","dateUpdated":"2020-03-04T12:30:41+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"succes\n"}]},"apps":[],"jobName":"paragraph_1583324719302_1213591282","id":"20200229-125527_1750970141","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:30:41+0000","dateFinished":"2020-03-04T12:30:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16725"},{"text":"%pyspark\ndataRDD_translation.take(3)","user":"anonymous","dateUpdated":"2020-03-04T12:32:22+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o96.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3://gdelt2020/masterfilelist_translation.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:260)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:46)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o96.partitions.\\n', JavaObject id=o140), <traceback object at 0x7f7d4d227518>)"}]},"apps":[],"jobName":"paragraph_1583324719302_-1013871366","id":"20200229-125639_738706056","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:32:22+0000","dateFinished":"2020-03-04T12:32:23+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:16726"},{"text":"%pyspark\ndataRDD.take(3)","user":"anonymous","dateUpdated":"2020-03-04T14:54:26+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(hash=u'297a16b493de7cf6ca809a7cc31d0b93', size=u'150383', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip'), Row(hash=u'bb27f78ba45f69a17ea6ed7755e9f8ff', size=u'318084', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip'), Row(hash=u'ea8dde0beb0ba98810a92db068c0ce99', size=u'10768507', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip')]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=58"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719303_1231223541","id":"20200228-140543_955973419","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T14:54:26+0000","dateFinished":"2020-03-04T14:54:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16727"},{"text":"%pyspark\nrdd1 = dataRDD.filter(lambda e : \"mentions\" in e)","user":"anonymous","dateUpdated":"2020-03-04T15:23:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1583335245452_-42170545","id":"20200304-152045_85860300","dateCreated":"2020-03-04T15:20:45+0000","dateStarted":"2020-03-04T15:23:15+0000","dateFinished":"2020-03-04T15:23:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16728"},{"text":"%pyspark\nrdd1.take(5)","user":"anonymous","dateUpdated":"2020-03-04T15:23:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 4 times, most recent failure: Lost task 0.3 in stage 64.0 (TID 137, ip-172-31-45-37.ec2.internal, executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor357.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1352, in takeUpToNumLeft\n  File \"/mnt1/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000054/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n', JavaObject id=o1489), <traceback object at 0x7f7d4d2e4c20>)"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=64"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583335288452_-1600807560","id":"20200304-152128_648336239","dateCreated":"2020-03-04T15:21:28+0000","dateStarted":"2020-03-04T15:23:18+0000","dateFinished":"2020-03-04T15:23:30+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:16729"},{"text":"%pyspark\ndataDF = dataRDD.toDF()","user":"anonymous","dateUpdated":"2020-03-04T12:32:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719303_315767082","id":"20200228-150242_1865183849","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:32:35+0000","dateFinished":"2020-03-04T12:32:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16730"},{"text":"%pyspark\ndataDF.show(5)","user":"anonymous","dateUpdated":"2020-03-04T12:33:10+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------+--------------------+\n|                hash|    size|                 url|\n+--------------------+--------+--------------------+\n|297a16b493de7cf6c...|  150383|http://data.gdelt...|\n|bb27f78ba45f69a17...|  318084|http://data.gdelt...|\n|ea8dde0beb0ba9881...|10768507|http://data.gdelt...|\n|2a91041d7e72b0fc6...|  149211|http://data.gdelt...|\n|dec3f427076b716a8...|  339037|http://data.gdelt...|\n+--------------------+--------+--------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719304_1824720467","id":"20200229-114643_1302631653","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:33:10+0000","dateFinished":"2020-03-04T12:33:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16731"},{"text":"%pyspark\nurlDF = dataDF.select(\"url\").show(5, False)","user":"anonymous","dateUpdated":"2020-03-04T14:48:54+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------------------------------------------+\n|url                                                                 |\n+--------------------------------------------------------------------+\n|http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip  |\n|http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip|\n|http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip     |\n|http://data.gdeltproject.org/gdeltv2/20150218231500.export.CSV.zip  |\n|http://data.gdeltproject.org/gdeltv2/20150218231500.mentions.CSV.zip|\n+--------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=54"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719304_-81500293","id":"20200229-113851_1426233595","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T14:48:54+0000","dateFinished":"2020-03-04T14:48:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16732"},{"text":"%pyspark\nrdd2.take(5)","user":"anonymous","dateUpdated":"2020-03-04T14:48:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(hash=u'297a16b493de7cf6ca809a7cc31d0b93', size=u'150383', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip'), Row(hash=u'bb27f78ba45f69a17ea6ed7755e9f8ff', size=u'318084', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip'), Row(hash=u'ea8dde0beb0ba98810a92db068c0ce99', size=u'10768507', url=u'http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip'), Row(hash=u'2a91041d7e72b0fc6a629e2ff867b240', size=u'149211', url=u'http://data.gdeltproject.org/gdeltv2/20150218231500.export.CSV.zip'), Row(hash=u'dec3f427076b716a8112b9086c342523', size=u'339037', url=u'http://data.gdeltproject.org/gdeltv2/20150218231500.mentions.CSV.zip')]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=53"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583333292018_-1453035569","id":"20200304-144812_1823893711","dateCreated":"2020-03-04T14:48:12+0000","dateStarted":"2020-03-04T14:48:20+0000","dateFinished":"2020-03-04T14:48:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16733"},{"text":"%pyspark\n#from pyspark.sql.functions import *\n#newDf = dataDF.withColumn(\"url\", regexp_replace(\"url\",\"http://data.gdeltproject.org/gdeltv2/\" ,\"  \"  ))\n#newDf = dataDF.withColumn(\"url\", regexp_replace(\"url\",\"\" ,\"\"  ))","user":"anonymous","dateUpdated":"2020-03-04T12:25:19+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1583324719304_2013860956","id":"20200229-114827_752272535","dateCreated":"2020-03-04T12:25:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16734"},{"text":"%pyspark\nnewDf.show(5, False)","user":"anonymous","dateUpdated":"2020-03-04T12:34:04+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 1: newDf.show(5, False)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7541932691734872676.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'newDf' is not defined\n"}]},"apps":[],"jobName":"paragraph_1583324719305_-788320698","id":"20200229-122039_1054720183","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:34:04+0000","dateFinished":"2020-03-04T12:34:04+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:16735"},{"text":"%pyspark\nfrom pyspark.sql.functions import col\nnewDf_export= dataDF.filter(col(\"url\").like(\"%export%\")).show(5, False)","user":"anonymous","dateUpdated":"2020-03-04T12:35:01+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------+------+------------------------------------------------------------------+\n|hash                            |size  |url                                                               |\n+--------------------------------+------+------------------------------------------------------------------+\n|297a16b493de7cf6ca809a7cc31d0b93|150383|http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip|\n|2a91041d7e72b0fc6a629e2ff867b240|149211|http://data.gdeltproject.org/gdeltv2/20150218231500.export.CSV.zip|\n|12268e821823aae2da90882621feda18|149723|http://data.gdeltproject.org/gdeltv2/20150218233000.export.CSV.zip|\n|a5298ce3c6df1a8a759c61b5c0b6f8bb|158842|http://data.gdeltproject.org/gdeltv2/20150218234500.export.CSV.zip|\n|c4268d558bb22c02b3c132c17818c68b|362610|http://data.gdeltproject.org/gdeltv2/20150219000000.export.CSV.zip|\n+--------------------------------+------+------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=4"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719305_364172236","id":"20200229-114122_1260294947","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T12:35:02+0000","dateFinished":"2020-03-04T12:35:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16736"},{"text":"%pyspark\nfrom pyspark.sql.functions import col\nnewDf_mentions= dataDF.filter(col(\"url\").like(\"%mentions%\")).show(5, False)","user":"anonymous","dateUpdated":"2020-03-04T15:14:54+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------+------+--------------------------------------------------------------------+\n|hash                            |size  |url                                                                 |\n+--------------------------------+------+--------------------------------------------------------------------+\n|bb27f78ba45f69a17ea6ed7755e9f8ff|318084|http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip|\n|dec3f427076b716a8112b9086c342523|339037|http://data.gdeltproject.org/gdeltv2/20150218231500.mentions.CSV.zip|\n|744acad14559f2781a8db67715d63872|357229|http://data.gdeltproject.org/gdeltv2/20150218233000.mentions.CSV.zip|\n|dd322c888f28311aca2c735468405551|374528|http://data.gdeltproject.org/gdeltv2/20150218234500.mentions.CSV.zip|\n|e7f464a7a451ad2af6e9c8fa24f0ccea|287807|http://data.gdeltproject.org/gdeltv2/20150219000000.mentions.CSV.zip|\n+--------------------------------+------+--------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=61"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583324719306_1414765329","id":"20200229-124726_1013822176","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T15:14:54+0000","dateFinished":"2020-03-04T15:15:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16737"},{"text":"%pyspark\nfrom pyspark.sql.functions import col\nnewDf_2019= dataDF.filter(col(\"url\").like(\"%/2019%\")).show(5, False)","user":"anonymous","dateUpdated":"2020-03-04T15:44:32+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o1578.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 4 times, most recent failure: Lost task 0.3 in stage 68.0 (TID 147, ip-172-31-44-37.ec2.internal, executor 29): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor312.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1583324376881_0001/container_1583324376881_0001_01_000058/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 4, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o1578.showString.\\n', JavaObject id=o1579), <traceback object at 0x7f7d4d2e4878>)"}]},"apps":[],"jobName":"paragraph_1583324719306_-79194330","id":"20200229-114305_65284479","dateCreated":"2020-03-04T12:25:19+0000","dateStarted":"2020-03-04T15:44:32+0000","dateFinished":"2020-03-04T15:44:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:16738","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-185.ec2.internal:4040/jobs/job?id=68"],"interpreterSettingId":"spark"}}},{"text":"\n\n","user":"anonymous","dateUpdated":"2020-03-04T15:12:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:1: error: '.' expected but identifier found.\nimport pandas as pd\n              ^\n"}]},"apps":[],"jobName":"paragraph_1583333144519_1548130118","id":"20200304-144544_30429720","dateCreated":"2020-03-04T14:45:44+0000","dateStarted":"2020-03-04T15:11:57+0000","dateFinished":"2020-03-04T15:11:57+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:16739"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583334580459_-556706978","id":"20200304-150940_239260431","dateCreated":"2020-03-04T15:09:40+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:16740"}],"name":"/esem/Gdelt_2020","id":"2F3S8QWWH","noteParams":{},"noteForms":{},"angularObjects":{"python:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}